{"cells":[{"cell_type":"markdown","metadata":{"id":"jbPBDAS3mQ5W"},"source":["# AnonED: Complex Region Anonymisation in Electrical Diagrams using Hybrid Density-Based Spatial Clustering\n","\n","### **Note**: Replace file paths, tessaract ocr language settings and OpenAI key in the notebook"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15146,"status":"ok","timestamp":1755100156872,"user":{"displayName":"Olumayowa Onabanjo","userId":"03188874242730545723"},"user_tz":-120},"id":"XXBosqW__SUj","outputId":"133d5e1c-42e9-4bf1-819a-03afd524c1cd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"2JYXfNFg_SQw","executionInfo":{"status":"ok","timestamp":1755100156916,"user_tz":-120,"elapsed":42,"user":{"displayName":"Olumayowa Onabanjo","userId":"03188874242730545723"}}},"outputs":[],"source":["import os\n","\n","# Define the target directory\n","target_directory = '/content/drive/MyDrive'\n","\n","# Create the directory if it doesn't exist\n","os.makedirs(target_directory, exist_ok=True)"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":23864,"status":"ok","timestamp":1755100182131,"user":{"displayName":"Olumayowa Onabanjo","userId":"03188874242730545723"},"user_tz":-120},"id":"WBe9OEA5_SM8","outputId":"d562a3f4-3f3e-41f6-9d4a-0c60231e69fa"},"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","tesseract-ocr is already the newest version (4.1.1-2.1build1).\n","0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n","Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.11/dist-packages (4.12.0.88)\n","Requirement already satisfied: numpy<2.3.0,>=2 in /usr/local/lib/python3.11/dist-packages (from opencv-python-headless) (2.0.2)\n","Collecting pytesseract\n","  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (25.0)\n","Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (11.3.0)\n","Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n","Installing collected packages: pytesseract\n","Successfully installed pytesseract-0.3.13\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n","Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n","Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (2024.11.6)\n"]}],"source":["# installation of dependencies\n","\n","!apt-get install -y tesseract-ocr\n","%pip install opencv-python-headless\n","%pip install pytesseract\n","%pip install pandas\n","%pip install regex"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"5uqVZqI4mBHC","executionInfo":{"status":"ok","timestamp":1755100185546,"user_tz":-120,"elapsed":3411,"user":{"displayName":"Olumayowa Onabanjo","userId":"03188874242730545723"}}},"outputs":[],"source":["# import libraries\n","\n","import cv2\n","import pytesseract\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from pytesseract import Output\n","import re\n","from scipy.spatial.distance import euclidean\n","from sklearn.cluster import DBSCAN"]},{"cell_type":"markdown","metadata":{"id":"stcR4MzsB5ef"},"source":["## download Tesseract OCR for your language(s) required"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":3000,"status":"ok","timestamp":1755100188535,"user":{"displayName":"Olumayowa Onabanjo","userId":"03188874242730545723"},"user_tz":-120},"id":"ocB8vzke_SJX","outputId":"49fb91a0-f841-434c-d3a4-e7c3c4efe05e"},"outputs":[{"output_type":"stream","name":"stdout","text":["configs  eng.traineddata  osd.traineddata  pdf.ttf  tessconfigs\n","--2025-08-13 15:49:45--  https://github.com/tesseract-ocr/tessdata/raw/main/spa.traineddata\n","Resolving github.com (github.com)... 140.82.114.3\n","Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://raw.githubusercontent.com/tesseract-ocr/tessdata/main/spa.traineddata [following]\n","--2025-08-13 15:49:45--  https://raw.githubusercontent.com/tesseract-ocr/tessdata/main/spa.traineddata\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 18256019 (17M) [application/octet-stream]\n","Saving to: ‘/usr/share/tesseract-ocr/4.00/tessdata/spa.traineddata’\n","\n","spa.traineddata     100%[===================>]  17.41M   113MB/s    in 0.2s    \n","\n","2025-08-13 15:49:45 (113 MB/s) - ‘/usr/share/tesseract-ocr/4.00/tessdata/spa.traineddata’ saved [18256019/18256019]\n","\n","--2025-08-13 15:49:45--  https://github.com/tesseract-ocr/tessdata/raw/main/cat.traineddata\n","Resolving github.com (github.com)... 140.82.114.3\n","Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://raw.githubusercontent.com/tesseract-ocr/tessdata/main/cat.traineddata [following]\n","--2025-08-13 15:49:46--  https://raw.githubusercontent.com/tesseract-ocr/tessdata/main/cat.traineddata\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 6502025 (6.2M) [application/octet-stream]\n","Saving to: ‘/usr/share/tesseract-ocr/4.00/tessdata/cat.traineddata’\n","\n","cat.traineddata     100%[===================>]   6.20M  --.-KB/s    in 0.1s    \n","\n","2025-08-13 15:49:47 (58.1 MB/s) - ‘/usr/share/tesseract-ocr/4.00/tessdata/cat.traineddata’ saved [6502025/6502025]\n","\n","List of available languages (4):\n","cat\n","eng\n","osd\n","spa\n"]}],"source":["# Step 1: Verify and download the language data file (if necessary)\n","!ls /usr/share/tesseract-ocr/4.00/tessdata\n","!wget -P /usr/share/tesseract-ocr/4.00/tessdata https://github.com/tesseract-ocr/tessdata/raw/main/spa.traineddata # replace with dataset main language\n","!wget -P /usr/share/tesseract-ocr/4.00/tessdata https://github.com/tesseract-ocr/tessdata/raw/main/cat.traineddata # replace with dataset secondary language(s)\n","\n","\n","# Step 2: Set the TESSDATA_PREFIX environment variable\n","os.environ['TESSDATA_PREFIX'] = '/usr/share/tesseract-ocr/4.00/tessdata'\n","\n","# Step 3: Verify the installation\n","!tesseract --list-langs"]},{"cell_type":"markdown","metadata":{"id":"2WAg9_4TnVrz"},"source":["#### Clean Detected Text\n","This section contains the `clean_text` function, which is used to clean the detected text by removing unwanted characters while keeping letters with accents.\n","\n","##### The `create_keyword_patterns` function, which generates regular expression patterns for a list of keywords. These patterns are used to match keywords in the detected text during OCR processing."]},{"cell_type":"code","execution_count":6,"metadata":{"id":"WAp8HvVynryy","executionInfo":{"status":"ok","timestamp":1755100188560,"user_tz":-120,"elapsed":24,"user":{"displayName":"Olumayowa Onabanjo","userId":"03188874242730545723"}}},"outputs":[],"source":["# Function to clean detected text, keeping letters with accents\n","def clean_text(text):\n","    text = re.sub(r'[^a-zA-Z0-9áéíóòúÁÉÍÓÒÚñÑ\\s]', ' ', text)\n","    text = re.sub(r'\\s+', ' ', text).strip()\n","    return text\n","\n","# Function to create regex patterns for keywords\n","def create_keyword_patterns(keywords):\n","    patterns = []\n","    for keyword in keywords:\n","        pattern = r'\\b' + re.escape(keyword) + r'\\b'\n","        patterns.append(pattern)\n","    return patterns"]},{"cell_type":"markdown","metadata":{"id":"r0-zTTH9uRci"},"source":["### Approximate segments\n","This section contains the `appoximate_segments` function, it finds contours in the image using traditional image processing techniques. These will be used to confirm the OCR-based detected regions. This will help to cover parts of the title block that contain no text.\n","\n","#### step 1: preprocess image (grayscale, gaussian blur, adaptative threshold)\n","\n","#### step 2: morphological operations for horizontal and vertical lines\n","\n","#### step 3: extend lines by 1%\n","\n","#### step 4: detect contours\n","\n","#### step 5: separate large contours\n","\n","criteria for large contours: not exceed 15% of image area (for closed contours - derived from maximum ratio of title blocks) or 50% of image perimeter (for full-length title blocks on longest side)\n","\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"-Pr-X_5SuSe5","executionInfo":{"status":"ok","timestamp":1755100188643,"user_tz":-120,"elapsed":64,"user":{"displayName":"Olumayowa Onabanjo","userId":"03188874242730545723"}}},"outputs":[],"source":["def appoximate_segments(image_path):\n","    # Read the image\n","    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n","\n","    if image is None:\n","        print(f\"Error: Unable to open image file {image_path}\")\n","        return None\n","\n","    # Get image dimensions\n","    height, width = image.shape[:2]\n","\n","    # Apply Gaussian blur to reduce noise\n","    blurred_image = cv2.GaussianBlur(image, (5, 5), 0)\n","\n","    # Apply adaptive thresholding\n","    binary_image = cv2.adaptiveThreshold(blurred_image, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 11, 2)\n","\n","    k__pixel_length = int(min(height, width) * 0.01)\n","\n","    # Define kernels for morphological operations\n","    kernel_horizontal = np.ones((1, k__pixel_length), np.uint8)\n","    kernel_vertical = np.ones((k__pixel_length, 1), np.uint8)\n","\n","    # Detect horizontal lines\n","    horizontal_lines = cv2.morphologyEx(binary_image, cv2.MORPH_OPEN, kernel_horizontal)\n","\n","    # Detect vertical lines\n","    vertical_lines = cv2.morphologyEx(binary_image, cv2.MORPH_OPEN, kernel_vertical)\n","\n","    # Extend the length of detected horizontal and vertical lines by 1%\n","    horizontal_lines_extended = cv2.dilate(horizontal_lines, np.ones((1, int(horizontal_lines.shape[1] * 0.01))), iterations=1)\n","    vertical_lines_extended = cv2.dilate(vertical_lines, np.ones((int(vertical_lines.shape[0] * 0.01), 1)), iterations=1)\n","\n","    # Combine extended horizontal and vertical lines\n","    document_structure = cv2.add(horizontal_lines_extended, vertical_lines_extended)\n","\n","    # Find contours of the document structure\n","    contours, _ = cv2.findContours(document_structure, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_NONE)\n","\n","    # Initialize lists for large contours and remaining contours\n","    large_contours = []\n","    remaining_contours = []\n","\n","    # Filter contours based on area and perimeter\n","    for contour in contours:\n","        contour_area = cv2.contourArea(contour)\n","        x, y, w, h = cv2.boundingRect(contour)\n","        perimeter =  2 * (w) + (h)\n","        if contour_area > (0.15 * height * width) or perimeter > (0.5 * (height + width)):\n","            large_contours.append(contour)\n","        else:\n","            remaining_contours.append([[x, y, w, h]])\n","\n","    return remaining_contours"]},{"cell_type":"markdown","metadata":{"id":"d-NqgUmxn07p"},"source":["##### Mask most likely drawing sections\n","######This section contains the `mask_images` function, which processes images in the input folder and by masks regions where drawings are highly likely to reduce amout of OCR text to extract. The processed images are saved to another output directory."]},{"cell_type":"code","execution_count":8,"metadata":{"id":"2CtRAuSIoWBZ","executionInfo":{"status":"ok","timestamp":1755100188733,"user_tz":-120,"elapsed":40,"user":{"displayName":"Olumayowa Onabanjo","userId":"03188874242730545723"}}},"outputs":[],"source":["# Function to mask a rectangular region in the image with white pixels and save the processed images\n","def mask_images(input_directory, output_directory):\n","    # Create the output directory if it doesn't exist\n","    if not os.path.exists(output_directory):\n","        os.makedirs(output_directory)\n","\n","    # Loop through all files in the input directory\n","    for filename in os.listdir(input_directory):\n","        if filename.endswith(('.png', '.jpg', '.jpeg')):\n","            # Read the image\n","            image_path = os.path.join(input_directory, filename)\n","            image = cv2.imread(image_path)\n","\n","            # Get image dimensions\n","            height, width = image.shape[:2]\n","\n","            # Calculate the center point of the image\n","            center_x, center_y = width // 2, height // 2\n","\n","            # Calculate the dimensions of the rectangle (as a % of the image's area)\n","            rect_width = int(width * 0.5)\n","            rect_height = int(height * 0.4)\n","\n","            # Calculate the top-left and bottom-right points of the rectangle\n","            top_left_x = center_x - rect_width // 2 - int(rect_width * 0.2)\n","            top_left_y = center_y - rect_height // 2 - int(rect_height * 0.3)\n","            bottom_right_x = center_x + rect_width // 2\n","            bottom_right_y = center_y + rect_height // 2\n","\n","            # Mask the rectangular region with white pixels\n","            image[top_left_y:bottom_right_y, top_left_x:bottom_right_x] = (255, 255, 255)\n","\n","            # Save the processed image to the output directory\n","            output_path = os.path.join(output_directory, filename)\n","            cv2.imwrite(output_path, image)"]},{"cell_type":"markdown","metadata":{"id":"xSEMooRqoc_X"},"source":["##### Extract Bounding Boxes\n","This section includes the `extract_bounding_boxes` function, which extracts bounding box coordinates in PSM 5 & 12 on the original images with sharpening since this proved to detect as more bounding boxes than either of  the two methods. Words are used as tokens to match keywords in a provided dictionary."]},{"cell_type":"markdown","metadata":{"id":"1mNFzvoReRwo"},"source":["## Conditional sharpening based on Laplacian variance\n","\n","obtained threshold from dataset (below 4000, OCR improves for 72%, unchanged 22% and 6% worsen) (above threshold, OCR improves for 18%, unchanged 36% and 46% worsen)\n","\n","picked the threshold that gave the highest number of detected bounding boxes from the representative samples"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":4971,"status":"ok","timestamp":1755100193701,"user":{"displayName":"Olumayowa Onabanjo","userId":"03188874242730545723"},"user_tz":-120},"id":"z-k5YlWJdu6q","outputId":"78ab1701-f7d5-4a2b-8a41-220ca46bbb66"},"outputs":[{"output_type":"stream","name":"stdout","text":["a_9_page_1.png: 1766.4493936732333\n","a_14_page_1.png: 1820.3171495969002\n","a_2_page_1.png: 2096.288656056248\n","a_41_page_1.png: 3255.463095929233\n","a_16_page_1.png: 4311.51835523299\n"]}],"source":["def variance_of_laplacian(image):\n","    # Compute the Laplacian of the image and then return the variance\n","    return cv2.Laplacian(image, cv2.CV_64F).var()\n","\n","def calculate_sharpness_scores(directory):\n","    sharpness_scores = {}\n","    for filename in os.listdir(directory):\n","        if filename.endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n","            image_path = os.path.join(directory, filename)\n","            image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n","            if image is not None:\n","                score = variance_of_laplacian(image)\n","                sharpness_scores[filename] = score\n","    return sharpness_scores\n","\n","# Specify the directory containing the images\n","image_directory = '/content/drive/MyDrive/ICDAR_workshop/Github_AnonED/sample_EDs' # (REPLACE FILE PATH)\n","\n","# Calculate sharpness scores\n","sharpness_scores = calculate_sharpness_scores(image_directory)\n","sorted_by_values = dict(sorted(sharpness_scores.items(), key=lambda item: item[1]))\n","\n","# Print the sharpness scores\n","for filename, score in sorted_by_values.items():\n","    print(f'{filename}: {score}')"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"Ttd-UXM2dy5Z","executionInfo":{"status":"ok","timestamp":1755100193927,"user_tz":-120,"elapsed":84,"user":{"displayName":"Olumayowa Onabanjo","userId":"03188874242730545723"}}},"outputs":[],"source":["def sharpen_image(image):\n","    # Create a kernel for sharpening\n","    kernel = np.array([[0, -1, 0],\n","                       [-1, 5,-1],\n","                       [0, -1, 0]])\n","    # Apply the kernel to the image\n","    sharpened = cv2.filter2D(image, -1, kernel)\n","    return sharpened"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"iU1eF9-mo5ma","executionInfo":{"status":"ok","timestamp":1755100193967,"user_tz":-120,"elapsed":53,"user":{"displayName":"Olumayowa Onabanjo","userId":"03188874242730545723"}}},"outputs":[],"source":["# Function to extract bounding box coordinates for each matched keyword with combined PSM modes & languages\n","def extract_bounding_boxes(image, keywords):\n","    bounding_boxes = set()\n","\n","    keyword_patterns = create_keyword_patterns(keywords)\n","\n","    # Perform OCR on the original image\n","    # Check if the image is already in grayscale\n","    if len(image.shape) == 3:\n","        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","    else:\n","        gray_image = image\n","\n","    # Check the sharpness score and apply sharpening if necessary\n","    sharpness_score = sharpness_scores.get(filename, 0)\n","    if sharpness_score < 4000:\n","        gray_image = sharpen_image(gray_image)\n","\n","    for psm in [3, 5, 12]:\n","        config = f'--psm {psm}'\n","        data_dict_original = pytesseract.image_to_data(gray_image, config=config, lang='spa+cat', output_type=pytesseract.Output.DICT)\n","\n","        n_boxes_original = len(data_dict_original['level'])\n","\n","        for i in range(n_boxes_original):\n","            text_original = clean_text(data_dict_original['text'][i].lower())\n","            for pattern in keyword_patterns:\n","                if re.search(pattern, text_original):\n","                    bbox = (data_dict_original['left'][i], data_dict_original['top'][i], data_dict_original['width'][i], data_dict_original['height'][i])\n","                    bounding_boxes.add(bbox)\n","                    break\n","\n","    # Check if the image is already in grayscale\n","    if len(image.shape) == 3:\n","        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","    else:\n","        gray_image = image\n","\n","    # Apply adaptive thresholding\n","    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","    adapt_thresh_image = cv2.adaptiveThreshold(gray_image, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\n","\n","    # Perform OCR on the adaptively thresholded image\n","    for psm in [12]:\n","        config = f'--psm {psm}'\n","        data_dict_adapt_thresh = pytesseract.image_to_data(adapt_thresh_image, config=config, lang='spa+cat', output_type=pytesseract.Output.DICT)\n","\n","        n_boxes_adapt_thresh = len(data_dict_adapt_thresh['level'])\n","\n","        for i in range(n_boxes_adapt_thresh):\n","            text_adapt_thresh = clean_text(data_dict_adapt_thresh['text'][i].lower())\n","            for pattern in keyword_patterns:\n","                if re.search(pattern, text_adapt_thresh):\n","                    bbox = (data_dict_adapt_thresh['left'][i], data_dict_adapt_thresh['top'][i], data_dict_adapt_thresh['width'][i], data_dict_adapt_thresh['height'][i])\n","                    bounding_boxes.add(bbox)\n","                    break\n","\n","    return list(bounding_boxes)"]},{"cell_type":"markdown","metadata":{"id":"s-MAiIzxpRLZ"},"source":["##### Adapt DBSCAN Eps\n","##### This section contains the `adapt_dbscan_eps` function, which adapts the DBSCAN epsilon parameter based on the dimensions of the image. This ensures that the clustering algorithm works effectively for images of different sizes."]},{"cell_type":"code","execution_count":12,"metadata":{"id":"sOspvkz9pbnh","executionInfo":{"status":"ok","timestamp":1755100194097,"user_tz":-120,"elapsed":46,"user":{"displayName":"Olumayowa Onabanjo","userId":"03188874242730545723"}}},"outputs":[],"source":["# Function to adapt the DBSCAN eps based on the dimensions of each image\n","def adapt_dbscan_eps(image):\n","    height, width = image.shape[:2]\n","    longest_side = max(height, width)\n","    eps = longest_side\n","    return eps"]},{"cell_type":"markdown","metadata":{"id":"-XM_0V9Fxtde"},"source":["# extract bbox from feature-based"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"3A3a97eHxw_g","executionInfo":{"status":"ok","timestamp":1755100196118,"user_tz":-120,"elapsed":35,"user":{"displayName":"Olumayowa Onabanjo","userId":"03188874242730545723"}}},"outputs":[],"source":["def extract_bounding_boxes_from_contours(contours, image):\n","    bounding_boxes = []\n","    img_height, img_width = image.shape[:2]\n","    for contour in contours:\n","        if len(contour) > 0:  # Ensure the contour is not empty\n","            x, y, w, h = contour[0]  # Directly unpack the bounding box\n","            contour_area = w * h\n","            image_area = img_width * img_height\n","            if contour_area <= 0.5 * image_area:  # Ensure the contour is not greater than 50% of image area\n","              bounding_boxes.append([x, y, x + w, y + h])\n","    return bounding_boxes"]},{"cell_type":"markdown","metadata":{"id":"gE8EMp3zx4hK"},"source":["# extract bbox from OCR clusters"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"s1M7f83Fx7q4","executionInfo":{"status":"ok","timestamp":1755100197644,"user_tz":-120,"elapsed":32,"user":{"displayName":"Olumayowa Onabanjo","userId":"03188874242730545723"}}},"outputs":[],"source":["# Function to create bounding boxes from clusters\n","def create_bounding_boxes_from_clusters(image, final_clusters):\n","    cluster_boxes = []\n","    height, width = image.shape[:2]\n","    max_box_area = 0.15 * (height * width)\n","\n","    for cluster in final_clusters:\n","        x_coords = [bbox[0] for bbox in cluster]\n","        y_coords = [bbox[1] for bbox in cluster]\n","        w_coords = [bbox[2] for bbox in cluster]\n","        h_coords = [bbox[3] for bbox in cluster]\n","\n","        x_min = min(x_coords)\n","        y_min = min(y_coords)\n","        x_max = max(x_coords) + min(w_coords)\n","        y_max = max(y_coords) + min(h_coords)\n","\n","        # Calculate the area of the bounding box\n","        box_area = (x_max - x_min) * (y_max - y_min)\n","\n","        # Check if the bounding box area exceeds 15% of the image area\n","        if box_area < max_box_area:\n","            cluster_boxes.append([x_min, y_min, x_max, y_max])\n","        else:\n","            # If the box exceeds the limit, add individual boxes instead\n","            for bbox in cluster:\n","                cluster_boxes.append(bbox)\n","\n","    return cluster_boxes"]},{"cell_type":"markdown","metadata":{"id":"a6QtG0fKyEPp"},"source":["# remove feature-based contours that do not overlap with OCR-based areas"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"HfWpdlV2yBW-","executionInfo":{"status":"ok","timestamp":1755100199666,"user_tz":-120,"elapsed":51,"user":{"displayName":"Olumayowa Onabanjo","userId":"03188874242730545723"}}},"outputs":[],"source":["# Filter by overlap function\n","def filter_contours_by_overlap(remaining_contours, cluster_boxes, image, threshold=0):\n","    # Extract bounding boxes from remaining_contours\n","    remaining_boxes = extract_bounding_boxes_from_contours(remaining_contours, image)\n","\n","    # Create bounding boxes around each of the clusters in final_clusters\n","    final_cluster_boxes_after_group = cluster_boxes\n","\n","    filtered_contours = []\n","\n","    def is_overlapping(box1, box2):\n","        x1_min, y1_min, x1_max, y1_max = box1\n","        x2_min, y2_min, x2_max, y2_max = box2\n","        return not (x1_max <= x2_min or x2_max <= x1_min or y1_max <= y2_min or y2_max <= y1_min)\n","\n","    for rbox in remaining_boxes:\n","        rx1, ry1, rx2, ry2 = rbox\n","        for fbox in final_cluster_boxes_after_group:\n","            fx1, fy1, fx2, fy2 = fbox\n","\n","            # Calculate the intersection area\n","            ix1 = max(rx1, fx1)\n","            iy1 = max(ry1, fy1)\n","            ix2 = min(rx2, fx2)\n","            iy2 = min(ry2, fy2)\n","\n","            iw = max(0, ix2 - ix1)\n","            ih = max(0, iy2 - iy1)\n","\n","            intersection_area = iw * ih\n","\n","            # Calculate the union area\n","            rbox_area = (rx2 - rx1) * (ry2 - ry1)\n","            fbox_area = (fx2 - fx1) * (fy2 - fy1)\n","\n","            union_area = rbox_area + fbox_area - intersection_area\n","\n","            # Calculate the Intersection over Union (IoU)\n","            if union_area == 0:\n","                iou = 0\n","            else:\n","                iou = intersection_area / union_area\n","\n","            if iou > threshold or is_overlapping(rbox, fbox):\n","                filtered_contours.append(rbox)\n","                break\n","\n","    return filtered_contours"]},{"cell_type":"markdown","metadata":{"id":"yohqHP_Hw8TX"},"source":["### union of  feature-based and OCR-based"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"2ZpOW5uEw8pq","executionInfo":{"status":"ok","timestamp":1755100202152,"user_tz":-120,"elapsed":32,"user":{"displayName":"Olumayowa Onabanjo","userId":"03188874242730545723"}}},"outputs":[],"source":["def combine_and_merge(image_path, cluster_boxes):\n","    # Load the image to get its dimensions\n","    image = cv2.imread(image_path)\n","    img_height, img_width = image.shape[:2]\n","\n","    remaining_contours = appoximate_segments(image_path)\n","\n","    # Filter contours in remaining_contours by overlap and proximity to final_clusters\n","    filtered_contours = filter_contours_by_overlap(remaining_contours, cluster_boxes, image)\n","\n","    # Create bounding boxes around each of the clusters in final_clusters\n","    final_cluster_boxes_after_group = cluster_boxes\n","\n","    # Combine filtered_contours with final_cluster_boxes_after_group\n","    all_boxes_list = filtered_contours + final_cluster_boxes_after_group\n","\n","    return all_boxes_list"]},{"cell_type":"markdown","metadata":{"id":"cjI9bGrBq_-E"},"source":["### Group keywords by orientation\n","This section calls the `group_keywords_and_check_size` function. It ensures images that are larger than the threshold in the previous step are classified (2 categories - horizontal & vertical) and regrouped using DBSCAN within a smaller ROI. Outliers are removed by calculating Euclidean distance until the title block ROI is within the threshold."]},{"cell_type":"code","execution_count":17,"metadata":{"id":"M_QgjIDfsMRL","executionInfo":{"status":"ok","timestamp":1755100204070,"user_tz":-120,"elapsed":2,"user":{"displayName":"Olumayowa Onabanjo","userId":"03188874242730545723"}}},"outputs":[],"source":["# Function to group keywords by their corrected bounding box orientation and check size requirements\n","def group_keywords_and_check_size(bounding_boxes, img_height, img_width):\n","    keyword_groups_by_orientation = {'horizontal': [], 'vertical': []}\n","\n","    # Group keywords by their corrected bounding box orientation\n","    for bbox in bounding_boxes:\n","        width, height = bbox[2], bbox[3]\n","        if width >= height:\n","            keyword_groups_by_orientation['horizontal'].append(bbox)\n","        else:\n","            keyword_groups_by_orientation['vertical'].append(bbox)\n","\n","    # Check size requirements and split clusters if necessary\n","    final_clusters_by_orientation = []\n","\n","    for group in keyword_groups_by_orientation.values():\n","        if not group:\n","            continue\n","        points = np.array([(bbox[0] + bbox[2] / 2, bbox[1] + bbox[3] / 2) for bbox in group])\n","        eps = adapt_dbscan_eps(image)\n","        dbscan = DBSCAN(eps=0.35 * eps, min_samples=1).fit(points)\n","        labels = dbscan.labels_\n","\n","        clusters = {}\n","        for label, bbox in zip(labels, group):\n","            if label not in clusters:\n","                clusters[label] = []\n","            clusters[label].append(bbox)\n","\n","        for cluster in clusters.values():\n","            x_coords = [bbox[0] for bbox in cluster]\n","            y_coords = [bbox[1] for bbox in cluster]\n","            w_coords = [bbox[2] for bbox in cluster]\n","            h_coords = [bbox[3] for bbox in cluster]\n","\n","            x_min = min(x_coords)\n","            y_min = min(y_coords)\n","            x_max = max(x_coords) + min(w_coords)\n","            y_max = max(y_coords) + min(h_coords)\n","\n","            cluster_area = (x_max - x_min) * (y_max - y_min)\n","            min_cluster_area = 0.01 * img_height * img_width\n","            max_cluster_area = 0.085 * img_height * img_width\n","\n","            if cluster_area >= min_cluster_area and cluster_area <= max_cluster_area:\n","                final_clusters_by_orientation.append(cluster)\n","            else:\n","                # Successively remove outliers until the group's size meets the size requirements\n","                while cluster_area >= max_cluster_area and len(cluster) != 0:\n","                    cluster_center = np.median([(bbox[0] + bbox[2] / 2, bbox[1] + bbox[3] / 2) for bbox in cluster], axis=0)\n","                    points = np.array([(bbox[0] + bbox[2] / 2, bbox[1] + bbox[3] / 2) for bbox in cluster])\n","\n","                    distances = [euclidean(point, cluster_center) for point in points]\n","\n","                    farthest_index = np.argmax(distances)\n","                    farthest_bbox = cluster.pop(farthest_index)\n","\n","                    x_coords = [bbox[0] for bbox in cluster]\n","                    y_coords = [bbox[1] for bbox in cluster]\n","                    w_coords = [bbox[2] for bbox in cluster]\n","                    h_coords = [bbox[3] for bbox in cluster]\n","\n","                    x_min = min(x_coords)\n","                    y_min = min(y_coords)\n","                    x_max = max(x_coords) + min(w_coords)\n","                    y_max = max(y_coords) + max(h_coords)\n","\n","                    cluster_area = (x_max - x_min) * (y_max - y_min)\n","\n","                final_clusters_by_orientation.append(cluster)\n","\n","    return final_clusters_by_orientation"]},{"cell_type":"markdown","metadata":{"id":"RQm917QBqBfl"},"source":["### Draw Bounding Boxes\n","This section contains the `draw_bounding_boxes` function, which draws bounding boxes around clusters and isolated keywords on the image. The function uses the results of the non-maximum suppression to ensure that the bounding boxes are properly drawn."]},{"cell_type":"code","execution_count":18,"metadata":{"id":"DxLrd6pRqKXu","executionInfo":{"status":"ok","timestamp":1755100207602,"user_tz":-120,"elapsed":37,"user":{"displayName":"Olumayowa Onabanjo","userId":"03188874242730545723"}}},"outputs":[],"source":["def draw_bounding_boxes(image, all_boxes_list, final_clusters_by_orientation):\n","    # Function to check if two boxes overlap or touch\n","    def is_overlapping(box1, box2):\n","        x1_min, y1_min, x1_max, y1_max = box1\n","        x2_min, y2_min, x2_max, y2_max = box2\n","        return not (x1_max < x2_min or x2_max < x1_min or y1_max < y2_min or y2_max < y1_min)\n","\n","    # Function to merge overlapping boxes into one\n","    def merge_boxes(boxes):\n","        if not boxes:\n","            return []\n","        x_min = min(box[0] for box in boxes)\n","        y_min = min(box[1] for box in boxes)\n","        x_max = max(box[2] for box in boxes)\n","        y_max = max(box[3] for box in boxes)\n","        return [x_min, y_min, x_max, y_max]\n","\n","    # Function to find clusters of overlapping boxes\n","    def find_clusters(boxes):\n","        clusters = []\n","        visited = set()\n","\n","        def dfs(box, cluster):\n","            for i, other_box in enumerate(boxes):\n","                if i not in visited and is_overlapping(box, other_box):\n","                    visited.add(i)\n","                    cluster.append(other_box)\n","                    dfs(other_box, cluster)\n","\n","        for i, box in enumerate(boxes):\n","            if i not in visited:\n","                cluster = [box]\n","                visited.add(i)\n","                dfs(box, cluster)\n","                clusters.append(cluster)\n","\n","        return clusters\n","\n","    # Find clusters of overlapping boxes\n","    clusters = find_clusters(all_boxes_list)\n","\n","    # Separate isolated boxes and merged clusters\n","    isolated_boxes = [cluster[0] for cluster in clusters if len(cluster) == 1]\n","    merged_boxes = [merge_boxes(cluster) for cluster in clusters if len(cluster) > 1]\n","\n","    # Ensure the size of any box in merged_boxes cannot exceed 15% of the image area\n","    image_area = image.shape[0] * image.shape[1]\n","    max_box_area = 0.15 * image_area\n","\n","    valid_isolated_boxes = []\n","    for i in range(len(isolated_boxes)):\n","        x_min, y_min, x_max, y_max = isolated_boxes[i]\n","\n","        box_area = (x_max - x_min) * (y_max - y_min)\n","        if box_area < max_box_area:\n","            width_increase = int((x_max - x_min) * 0.005)\n","            height_increase = int((y_max - y_min) * 0.01)\n","            new_x_min = x_min - width_increase\n","            new_y_min = y_min - height_increase\n","            new_x_max = x_max + (width_increase * 2)\n","            new_y_max = y_max + (height_increase * 4)\n","            valid_isolated_boxes.append((new_x_min, new_y_min, new_x_max, new_y_max))\n","        else:\n","            # Use create_bounding_boxes_from_clusters function to create cluster_boxes from final_clusters_by_orientation\n","            cluster_boxes = create_bounding_boxes_from_clusters(image, final_clusters_by_orientation)\n","            valid_isolated_boxes.extend(cluster_boxes)\n","\n","    valid_merged_boxes = []\n","    for i in range(len(merged_boxes)):\n","        x_min, y_min, x_max, y_max = merged_boxes[i]\n","\n","        box_area = (x_max - x_min) * (y_max - y_min)\n","        if box_area < max_box_area:\n","            width_increase = int((x_max - x_min) * 0.005)\n","            height_increase = int((y_max - y_min) * 0.01)\n","            new_x_min = x_min - width_increase\n","            new_y_min = y_min - height_increase\n","            new_x_max = x_max + (width_increase * 2)\n","            new_y_max = y_max + (height_increase * 4)\n","            valid_merged_boxes.append((new_x_min, new_y_min, new_x_max, new_y_max))\n","        else:\n","            # Use create_bounding_boxes_from_clusters function to create cluster_boxes from final_clusters_by_orientation\n","            cluster_boxes = create_bounding_boxes_from_clusters(image, final_clusters_by_orientation)\n","            valid_isolated_boxes.extend(cluster_boxes)\n","\n","    # Draw the final bounding boxes on the image\n","    for (x_min, y_min, x_max, y_max) in valid_isolated_boxes + valid_merged_boxes:\n","        cv2.rectangle(image, (x_min, y_min), (x_max, y_max), (255, 0, 255), 3)"]},{"cell_type":"markdown","metadata":{"id":"eZ5sk6194dSc"},"source":["# process image to get final_clusters"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"-2cqtu704Zau","executionInfo":{"status":"ok","timestamp":1755100211727,"user_tz":-120,"elapsed":16,"user":{"displayName":"Olumayowa Onabanjo","userId":"03188874242730545723"}}},"outputs":[],"source":["# Function to process image and return final_clusters\n","def process_image(image, masked_image, keywords, img_height, img_width):\n","    # Extract bounding boxes for matched keywords from the masked images\n","    bounding_boxes = extract_bounding_boxes(masked_image, keywords)\n","\n","    # Convert bounding boxes to points for DBSCAN\n","    points = np.array([(bbox[0] + bbox[2] / 2, bbox[1] + bbox[3] / 2) for bbox in bounding_boxes])\n","\n","    # Perform DBSCAN clustering\n","    eps = adapt_dbscan_eps(image)\n","    dbscan = DBSCAN(eps=eps, min_samples=1).fit(points)\n","    labels = dbscan.labels_\n","\n","    clusters = {}\n","    for label, bbox in zip(labels, bounding_boxes):\n","        if label not in clusters:\n","            clusters[label] = []\n","        clusters[label].append(bbox)\n","\n","    # Check cluster sizes and adjust if necessary\n","    final_clusters = []\n","    for cluster in clusters.values():\n","        x_coords = [bbox[0] for bbox in cluster]\n","        y_coords = [bbox[1] for bbox in cluster]\n","        w_coords = [bbox[2] for bbox in cluster]\n","        h_coords = [bbox[3] for bbox in cluster]\n","\n","        x_min = min(x_coords)\n","        y_min = min(y_coords)\n","        x_max = max(x_coords) + min(w_coords)\n","        y_max = max(y_coords) + min(h_coords)\n","\n","        cluster_area = (x_max - x_min) * (y_max - y_min)\n","        max_cluster_area = 0.15 * img_height * img_width\n","\n","        if cluster_area < max_cluster_area:\n","            final_clusters.append(cluster)\n","        else:\n","            # Regroup the cluster into 2 groups using group_keywords_and_check_size function\n","            sub_clusters = group_keywords_and_check_size(cluster, img_height, img_width)\n","            final_clusters.extend(sub_clusters)\n","\n","    return final_clusters"]},{"cell_type":"markdown","metadata":{"id":"hBRyUwDdC-BQ"},"source":["# Combined script"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1wFoGy3R1tW-","executionInfo":{"status":"ok","timestamp":1755100436402,"user_tz":-120,"elapsed":214177,"user":{"displayName":"Olumayowa Onabanjo","userId":"03188874242730545723"}},"outputId":"089cc372-06e2-4137-e337-00de024c1170"},"outputs":[{"output_type":"stream","name":"stdout","text":["Image: a_16_page_1.png, Number of bounding boxes: 43\n","Image: a_2_page_1.png, Number of bounding boxes: 41\n","Image: a_14_page_1.png, Number of bounding boxes: 12\n","Image: a_41_page_1.png, Number of bounding boxes: 11\n","Image: a_9_page_1.png, Number of bounding boxes: 6\n","Processing complete.\n"]}],"source":["# Main script # (REPLACE FILE PATHS)\n","input_dir = '/content/drive/MyDrive/ICDAR_workshop/Github_AnonED/sample_EDs'\n","output_dir_final_images = '/content/drive/MyDrive/ICDAR_workshop/Github_AnonED/predictions'\n","output_dir_masked_images = '/content/drive/MyDrive/ICDAR_workshop/Github_AnonED/mask_applied'\n","keywords_file_path = '/content/drive/MyDrive/ICDAR_workshop/Github_AnonED/4o_mini_dict.txt'\n","\n","# Create output directories if they don't exist\n","os.makedirs(output_dir_final_images, exist_ok=True)\n","os.makedirs(output_dir_masked_images, exist_ok=True)\n","\n","# Load keywords from text file\n","with open(keywords_file_path, 'r') as file:\n","    keywords = [re.sub(r'[^a-zA-Z0-9áéíóòúÁÉÍÓÒÚñÑ\\\\s]', ' ', line).strip().lower() for line in file]\n","\n","# Mask images and save to the masked images directory\n","mask_images(input_dir, output_dir_masked_images)\n","\n","# Process images in the input directory\n","for filename in os.listdir(input_dir):\n","    if filename.endswith('.png') or filename.endswith('.jpg'):\n","        image_path = os.path.join(input_dir, filename)\n","        masked_image_path = os.path.join(output_dir_masked_images, filename)\n","\n","        image = cv2.imread(image_path)\n","        masked_image = cv2.imread(masked_image_path)\n","        img_height, img_width = image.shape[:2]\n","\n","        # Extract bounding boxes with combined PSM modes & languages\n","        bounding_boxes = extract_bounding_boxes(masked_image, keywords)\n","        # Count the number of bounding boxes\n","        num_bounding_boxes = len(bounding_boxes)\n","        print(f\"Image: {filename}, Number of bounding boxes: {num_bounding_boxes}\")\n","\n","        # Process the image and get final_clusters\n","        final_clusters = process_image(image, masked_image, keywords, img_height, img_width)\n","\n","        # Get clusters separated by orientation\n","        final_clusters_by_orientation = group_keywords_and_check_size(bounding_boxes, img_height, img_width)\n","\n","        # Create bounding boxes from clusters\n","        cluster_boxes = create_bounding_boxes_from_clusters(image, final_clusters_by_orientation)\n","\n","        # Combine and merge bounding boxes\n","        all_boxes_list = combine_and_merge(image_path, cluster_boxes)\n","\n","        # Draw final bounding boxes\n","        draw_bounding_boxes(image, all_boxes_list, final_clusters_by_orientation)\n","\n","        # Save the final image with drawn bounding boxes\n","        final_image_path = os.path.join(output_dir_final_images, filename)\n","        cv2.imwrite(final_image_path, image)\n","\n","print(\"Processing complete.\")"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNe39mqd2etc5tS+y9JtlQz"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}